{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project Programming of Bioinformatics\n",
    "## Group 1  20211\n",
    "### Authors:\n",
    "TAN WEN YIT (A18CS0259) <br>\n",
    "LIM JIA YEE (A18CS) <br>\n",
    "KAM SHWU CHIN (A18CS) <br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title: Application of Deep Learning in Diagnosis of HCC Through CT Scan Images\n",
    "The application of Deep Learning is essential in solving current problems in the industries such as recommendation algorithm (e.g. YouTube Recommendation Algorithm), speech recognition, object recognition, gestures and etc. In this mini project, we would love to demonstrate the diagnosis of HCC using Convolutional Neural Network (CNN) by using **Keras** (Tensorflow as framework)\n",
    "<hr>\n",
    "<b>Make sure you have TensorFlow/TensorFlow for GPU, Keras, Numpy, Pandas, Skimage installed in your environment through Anaconda</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import essential packages\n",
    "**Keras** has packages to build up a CNN whereas **Numpy** is imported to manage with images.<br>\n",
    "**Pandas** is imported to read dataframe of the list of the images we have prepared as well as their labels.<br>\n",
    "**Skimage** is imported as well to read image in grayscale and normalize the value from 0 - 255 to 0 - 1.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from skimage import io\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import the images and labels using .csv file as guide\n",
    "The metadata of the images are stored in .csv file.<br>\n",
    "We need to fetch it by using **Pandas** package.<br>\n",
    "Then, we will convert the dataframe into **numpy** array because we will use arrays as a references of the images we need to fetch.<br><br>\n",
    "\n",
    "Images are read as grayscale images and all have equal size (512 x 512).<br>\n",
    "The images are stored in 3D array (1 x 512 x 512), so when we stack up the array we will have (total_num_of_images x 512 x 512).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading images...\n",
      "Images reading completed.\n"
     ]
    }
   ],
   "source": [
    "# read .csv file\n",
    "img_info_df = pd.read_csv('HCC_datasets_w_ohe.csv')\n",
    "img_name_df = pd.DataFrame(img_info_df['Images'], columns=['Images'])\n",
    "labels_df = pd.DataFrame(img_info_df['HCC_Yes'], columns=['HCC_Yes'])\n",
    "\n",
    "# flattening array into 1-D\n",
    "images_name_all = img_name_df.to_numpy().reshape(-1)\n",
    "labels_all = labels_df.to_numpy().reshape(-1)\n",
    "images_all = np.empty([1, 512, 512]) # later we need to remove the first element\n",
    "# print(images_all.shape)\n",
    "\n",
    "print(\"Reading images...\")\n",
    "# read images into ndarray with dimensions (total_images, 512, 512, 1)\n",
    "for image in images_name_all:\n",
    "    tempName = 'HCC_datasets/' + image\n",
    "    img_gs = io.imread(tempName, as_gray=True)\n",
    "    img_gs = img_gs.reshape((1, 512, 512))\n",
    "    images_all = np.vstack((images_all, img_gs))\n",
    "\n",
    "print(\"Images reading completed.\")\n",
    "# print(images_all.shape)\n",
    "images_all = np.delete(images_all, 0, axis=0) # remove the first element which contains empty array\n",
    "images_all = images_all.reshape(images_name_all.shape[0], 512, 512, 1) # reshape the array for CNN odel later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocess the datasets and split into training and testing set (parallel arrays)\n",
    "We need to split the images into **training set** and **training set**.<br>\n",
    "We will use normal distribution to randomize a set of random number.<br>\n",
    "Then, we will get the percentile (70%) from the set of numbers obtained.<br>\n",
    "For each number in the set, if the number is less than percentile 70%, then it will become <code>True</code> and vice versa.<br>\n",
    "Eventually, we have a filter mask with random selection of 70% to become training set.<br>\n",
    "Filter mask of 30% testing set will be the complement of the filter mask of training set using loop.<br>\n",
    "Last but not least, we have to categorize the **label**, our label are 0 (HCC absent) and 1 (HCC present).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create filter that splits datasets to 70/30\n",
    "rand_num_list = np.random.normal(size=(images_name_all.shape[0]))\n",
    "boundary = np.percentile(rand_num_list, 70)\n",
    "train_filter, test_filter = [], []\n",
    "true_num, false_num = 0, 0\n",
    "\n",
    "# create filter mask for training set\n",
    "for num in rand_num_list:\n",
    "    if num < boundary:\n",
    "        train_filter.append(True)\n",
    "        true_num += 1\n",
    "    else:\n",
    "        train_filter.append(False)\n",
    "        false_num += 1\n",
    "\n",
    "# create filter mask for testing set based on training filter\n",
    "for boo in train_filter:\n",
    "    if boo:\n",
    "        test_filter.append(False)\n",
    "    else:\n",
    "        test_filter.append(True)\n",
    "\n",
    "# filter the datasets into training and testing set\n",
    "training_img_name = images_name_all[train_filter]\n",
    "training_sets = images_all[train_filter]\n",
    "training_labels = labels_all[train_filter]\n",
    "\n",
    "testing_img_name = images_name_all[test_filter]\n",
    "testing_sets = images_all[test_filter]\n",
    "testing_labels = labels_all[test_filter]\n",
    "\n",
    "# categorize the label into YES(1) and NO(0)\n",
    "c_training_labels = np_utils.to_categorical(training_labels, 2)\n",
    "c_testing_labels = np_utils.to_categorical(testing_labels, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define hyperparameters\n",
    "**Hyperparameters** are the parameters that can be manually altered (or similar to **constant**). Here, we can set **feature map number**, or other parameters such as **kernel size**, **input_size**, and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_num = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build CNN models\n",
    "**Sequential()** is a framework to build up the neural network model.<br>\n",
    "<b>Convolutional2D(), MaxPooling2D, Dense()</b> are the layers in the **Sequential()**.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet = Sequential()\n",
    "convnet.add(Convolution2D(feature_map_num, 4, 4, activation='relu', input_shape=(512, 512, 1)))\n",
    "convnet.add(Convolution2D(feature_map_num, 3, 3, activation='relu'))\n",
    "convnet.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "convnet.add(Convolution2D(feature_map_num, 2, 2, activation='relu'))\n",
    "convnet.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "convnet.add(Convolution2D(feature_map_num, 2, 2, activation='relu'))\n",
    "convnet.add(Flatten())\n",
    "convnet.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Compile the model\n",
    "The optimizer used is called **Stochastic Gradient Descent** where the weights update are done after a random set from datasets are forward passed and backpropagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the summary of the model built such as output size of each layers and the number of parameters involved in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 128, 128, 64)      1088      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 42, 42, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 41, 41, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 19, 19, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 9, 9, 64)          16448     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5184)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 10370     \n",
      "=================================================================\n",
      "Total params: 81,282\n",
      "Trainable params: 81,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Number of weights after calling the model: 10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "convnet.summary()\n",
    "print(\"Number of weights after calling the model:\", len(convnet.weights), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Train the model with training set\n",
    "<code>batch_size</code> is the size of batch to train and update the weight of the model. We use 1 random sample per batch because our samples are fews only (140) and the training takes only a while to be done. If the samples is huge, then we have to specify larger batch size to perform weights update since we are using **Stochastic Gradient Descent**. (e.g. <code>batch_size = 64</code> indicating that the whole training set are just randomly picked 64 samples and being trained, forming an iteration. If we have, says 128 samples, then it takes 2 iterations to complete 1 **epoch**).<br><br>\n",
    "<i>The code below will perform 98 iterations per epoch, achieveing higher accuracy.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.2463 - accuracy: 0.5882\n",
      "Epoch 2/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.2431 - accuracy: 0.5809\n",
      "Epoch 3/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.2420 - accuracy: 0.5809\n",
      "Epoch 4/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.2375 - accuracy: 0.5882\n",
      "Epoch 5/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.2391 - accuracy: 0.5809\n",
      "Epoch 6/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.2342 - accuracy: 0.5809\n",
      "Epoch 7/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.2295 - accuracy: 0.5956\n",
      "Epoch 8/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.2230 - accuracy: 0.6176\n",
      "Epoch 9/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.2187 - accuracy: 0.6691\n",
      "Epoch 10/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.2066 - accuracy: 0.7059\n",
      "Epoch 11/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.1908 - accuracy: 0.7132\n",
      "Epoch 12/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.1825 - accuracy: 0.7647\n",
      "Epoch 13/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.1525 - accuracy: 0.8162\n",
      "Epoch 14/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.1302 - accuracy: 0.8309\n",
      "Epoch 15/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0944 - accuracy: 0.8971\n",
      "Epoch 16/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0756 - accuracy: 0.9191\n",
      "Epoch 17/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.9338\n",
      "Epoch 18/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9632\n",
      "Epoch 19/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0369 - accuracy: 0.9706\n",
      "Epoch 20/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0297 - accuracy: 0.9779\n",
      "Epoch 21/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.9853\n",
      "Epoch 22/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 0.9926\n",
      "Epoch 26/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 8.3978e-04 - accuracy: 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convnet.fit(training_sets, c_training_labels, batch_size=1, epochs=30, verbose=1)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluate the model using testing set\n",
    "We use the testing set to evaluate our model and tell us how accurate the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0244 - accuracy: 0.9661\n",
      "\n",
      "accuracy: 96.61%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = convnet.evaluate(testing_sets, c_testing_labels, verbose=1)\n",
    "print(\"\\n%s: %.2f%%\\n\" % (convnet.metrics_names[1], metrics[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Using the model to predict using testing set again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = convnet.predict(testing_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Get the overall result of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bc-a10w-11131992-1-12.jpg:  [0.02919318 0.97080684] Result: YES\n",
      "bc-a10w-11131992-1-13.jpg:  [0.00156978 0.9984302 ] Result: YES\n",
      "bc-a10x-03291993-1-14.jpg:  [8.952202e-04 9.991048e-01] Result: YES\n",
      "bc-a10z-12201993-01-36.jpg:  [0.00839722 0.9916027 ] Result: YES\n",
      "dd-a113-30984-2-010.jpg:  [0.54922193 0.45077804] Result: NO\n",
      "dd-a114-71147-1-42.jpg:  [0.00607056 0.9939294 ] Result: YES\n",
      "dd-a118-19398-1-12.jpg:  [0.00194709 0.99805295] Result: YES\n",
      "dd-a118-70938-1-12.jpg:  [0.09540807 0.90459186] Result: YES\n",
      "dd-a11a-55688-1-25.jpg:  [0.01575236 0.9842477 ] Result: YES\n",
      "dd-a11a-55688-1-26.jpg:  [0.02892043 0.9710796 ] Result: YES\n",
      "dd-a11a-55688-1-28.jpg:  [0.02987987 0.9701201 ] Result: YES\n",
      "dd-a11c-43276-1-036.jpg:  [0.00361865 0.9963813 ] Result: YES\n",
      "dd-a11c-43276-1-037.jpg:  [0.00550332 0.99449664] Result: YES\n",
      "dd-a11c-43276-1-038.jpg:  [0.00966781 0.9903322 ] Result: YES\n",
      "dd-a11c-43276-1-040.jpg:  [0.05782814 0.9421718 ] Result: YES\n",
      "dd-a11c-87471-1-037.jpg:  [0.1926442 0.8073558] Result: YES\n",
      "dd-a11d-37385-1-030.jpg:  [0.04747236 0.95252764] Result: YES\n",
      "dd-a11d-88127-1-029.jpg:  [0.00782423 0.99217576] Result: YES\n",
      "dd-a11d-88127-1-035.jpg:  [0.10188323 0.89811677] Result: YES\n",
      "dd-a1eg-01061998-1-051.jpg:  [0.02943887 0.9705611 ] Result: YES\n",
      "dd-a1eh-24559-1-028.jpg:  [0.03886608 0.96113396] Result: YES\n",
      "dd-a1eh-24559-1-030.jpg:  [0.01100442 0.98899555] Result: YES\n",
      "dd-a1ei-38159-1-034.jpg:  [0.02704997 0.97295004] Result: YES\n",
      "dd-a1ei-38159-1-035.jpg:  [0.02309536 0.97690463] Result: YES\n",
      "dd-a1ei-38159-1-036.jpg:  [0.02818818 0.97181183] Result: YES\n",
      "dd-a1ei-38159-1-039.jpg:  [0.022555 0.977445] Result: YES\n",
      "dd-a1el-04627-1-031.jpg:  [0.03927796 0.960722  ] Result: YES\n",
      "dd-a1el-28501-1-23.jpg:  [4.6869044e-04 9.9953127e-01] Result: YES\n",
      "dd-a4nf-36199-1-011.jpg:  [0.19300643 0.8069936 ] Result: YES\n",
      "dd-a4ng-64392-1-25.jpg:  [0.07081936 0.9291807 ] Result: YES\n",
      "dd-a4nq-15575-1-041.jpg:  [0.02123969 0.9787603 ] Result: YES\n",
      "K10-1-038.jpg:  [0.95040023 0.04959975] Result: NO\n",
      "K5-1-17.jpg:  [0.94459623 0.05540375] Result: NO\n",
      "B13-1-027.jpg:  [0.99839705 0.00160296] Result: NO\n",
      "B16-1-005.jpg:  [0.5554942  0.44450584] Result: NO\n",
      "B18-1-027.jpg:  [0.2153599  0.78464013] Result: YES\n",
      "B20-1-025.jpg:  [0.9960276  0.00397243] Result: NO\n",
      "B3-1-14.jpg:  [0.97425437 0.0257456 ] Result: NO\n",
      "B9-1-007.jpg:  [0.9925248  0.00747526] Result: NO\n",
      "L12-269.jpg:  [0.99846053 0.00153945] Result: NO\n",
      "L18-113.jpg:  [9.9987507e-01 1.2491431e-04] Result: NO\n",
      "L19-119.jpg:  [0.9536028 0.0463972] Result: NO\n",
      "L21-004.jpg:  [0.99818987 0.00181017] Result: NO\n",
      "L3-73.jpg:  [0.9538882  0.04611184] Result: NO\n",
      "L6-121.jpg:  [0.99698085 0.0030191 ] Result: NO\n",
      "L7-142.jpg:  [0.9716959  0.02830413] Result: NO\n",
      "A11-1-027.jpg:  [0.96402854 0.03597146] Result: NO\n",
      "A14-1-31.jpg:  [0.96788365 0.03211632] Result: NO\n",
      "A19-1-26.jpg:  [0.9701009  0.02989909] Result: NO\n",
      "A2-1-20.jpg:  [0.9806024  0.01939762] Result: NO\n",
      "A20-1-035.jpg:  [9.994980e-01 5.019241e-04] Result: NO\n",
      "A6-1-030.jpg:  [0.9966587  0.00334137] Result: NO\n",
      "A7-1-029.jpg:  [0.8221986  0.17780137] Result: NO\n",
      "A9-1-26.jpg:  [0.976548   0.02345204] Result: NO\n",
      "P10-1-038.jpg:  [0.5947527 0.4052473] Result: NO\n",
      "P12-1-046.jpg:  [0.9869665 0.0130335] Result: NO\n",
      "P7-1-29.jpg:  [0.98648214 0.01351786] Result: NO\n",
      "P8-1-032.jpg:  [9.9996638e-01 3.3625704e-05] Result: NO\n",
      "P9-1-033.jpg:  [9.999684e-01 3.154228e-05] Result: NO\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 59 is out of bounds for axis 0 with size 59",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-555e2403dd88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_filter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_img_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\": \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Result: NO\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 59 is out of bounds for axis 0 with size 59"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < len(test_filter):\n",
    "    if predictions[i][0] > predictions[i][1]:\n",
    "        print(testing_img_name[i] + \": \", predictions[i], \"Result: NO\")\n",
    "    else:\n",
    "        print(testing_img_name[i] + \": \", predictions[i], \"Result: YES\")\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
